{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ea945dd130>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax를 활용하여 숫자를 확률로 변환하는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.FloatTensor([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch has a softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "hypothesis = F.softmax(z, dim = 0)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " multi-class classification을 위해서는 Cross Entropy Loss를 써야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1664, 0.1871, 0.1737, 0.2695, 0.2033],\n",
      "        [0.2002, 0.1783, 0.2218, 0.1944, 0.2054],\n",
      "        [0.1809, 0.2380, 0.2318, 0.1084, 0.2409]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand(3,5,requires_grad = True)\n",
    "hypothesis = F.softmax(z, dim = 1)\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 3])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randint(5,(3,)).long()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 1., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_one_hot = torch.zeros_like(hypothesis)\n",
    "y_one_hot.scatter_(1,y.unsqueeze(1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6800, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim = 1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy Loss with torch.nn.functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7935, -1.6760, -1.7504, -1.3114, -1.5929],\n",
       "        [-1.6086, -1.7244, -1.5062, -1.6381, -1.5826],\n",
       "        [-1.7096, -1.4354, -1.4617, -2.2223, -1.4236]], grad_fn=<LogBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Low Lsvel\n",
    "torch.log(F.softmax(z,dim = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7935, -1.6760, -1.7504, -1.3114, -1.5929],\n",
       "        [-1.6086, -1.7244, -1.5062, -1.6381, -1.5826],\n",
       "        [-1.7096, -1.4354, -1.4617, -2.2223, -1.4236]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High Level\n",
    "F.log_softmax(z, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6800, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Low Lsvel\n",
    "(y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6800, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# High Level\n",
    "F.nll_loss(F.log_softmax(z, dim=1), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch also has F.cross_entropy that combines F.log_softmax() and f.nll_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.6800, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(z,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Low-level Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = [[1,2,1,1],\n",
    "           [2,1,3,2],\n",
    "           [3,1,3,4],\n",
    "           [4,1,5,5],\n",
    "           [1,7,5,5],\n",
    "           [1,2,5,6],\n",
    "           [1,6,6,6],\n",
    "           [1,7,7,7]]\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "y_train = torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 / 1000 Cost: 1.098612\n",
      "Epoch  100 / 1000 Cost: 0.761050\n",
      "Epoch  200 / 1000 Cost: 0.689991\n",
      "Epoch  300 / 1000 Cost: 0.643229\n",
      "Epoch  400 / 1000 Cost: 0.604117\n",
      "Epoch  500 / 1000 Cost: 0.568255\n",
      "Epoch  600 / 1000 Cost: 0.533922\n",
      "Epoch  700 / 1000 Cost: 0.500291\n",
      "Epoch  800 / 1000 Cost: 0.466908\n",
      "Epoch  900 / 1000 Cost: 0.433507\n",
      "Epoch 1000 / 1000 Cost: 0.399962\n"
     ]
    }
   ],
   "source": [
    "#모델 초기화\n",
    "W = torch.zeros((4,3),requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)\n",
    "\n",
    "#optimizer 설정\n",
    "optimizer = optim.SGD([W,b], lr = 0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    #cost 계산(1)\n",
    "    #hypothesis = F.softmax(x_train.matmul(W) + b, dim = 1)\n",
    "    #y_one_hot = torch.zeros_like(hypothesis)\n",
    "    #y_one_hot.scatter_(1, y_train.unsqueeze(1),1) #오류 원인 찾기\n",
    "    #cost = (y_one_hot * -torch.log(F.softmax(hypothesis,dim=1))).sum(dim=1) \n",
    "    \n",
    "    #cost 계산(2)\n",
    "    z = x_train.matmul(W) + b\n",
    "    cost = F.cross_entropy(z, y_train)\n",
    "    \n",
    "    #cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #100번 마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d} / {} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4,3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassifierModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0 / 1000 Cost: 2.288633\n",
      "Epoch   20 / 1000 Cost: 0.943391\n",
      "Epoch   40 / 1000 Cost: 0.801895\n",
      "Epoch   60 / 1000 Cost: 0.750842\n",
      "Epoch   80 / 1000 Cost: 0.719368\n",
      "Epoch  100 / 1000 Cost: 0.695717\n",
      "Epoch  120 / 1000 Cost: 0.676300\n",
      "Epoch  140 / 1000 Cost: 0.659513\n",
      "Epoch  160 / 1000 Cost: 0.644499\n",
      "Epoch  180 / 1000 Cost: 0.630745\n",
      "Epoch  200 / 1000 Cost: 0.617923\n",
      "Epoch  220 / 1000 Cost: 0.605808\n",
      "Epoch  240 / 1000 Cost: 0.594244\n",
      "Epoch  260 / 1000 Cost: 0.583114\n",
      "Epoch  280 / 1000 Cost: 0.572334\n",
      "Epoch  300 / 1000 Cost: 0.561838\n",
      "Epoch  320 / 1000 Cost: 0.551575\n",
      "Epoch  340 / 1000 Cost: 0.541505\n",
      "Epoch  360 / 1000 Cost: 0.531597\n",
      "Epoch  380 / 1000 Cost: 0.521824\n",
      "Epoch  400 / 1000 Cost: 0.512164\n",
      "Epoch  420 / 1000 Cost: 0.502601\n",
      "Epoch  440 / 1000 Cost: 0.493120\n",
      "Epoch  460 / 1000 Cost: 0.483707\n",
      "Epoch  480 / 1000 Cost: 0.474352\n",
      "Epoch  500 / 1000 Cost: 0.465047\n",
      "Epoch  520 / 1000 Cost: 0.455782\n",
      "Epoch  540 / 1000 Cost: 0.446552\n",
      "Epoch  560 / 1000 Cost: 0.437349\n",
      "Epoch  580 / 1000 Cost: 0.428169\n",
      "Epoch  600 / 1000 Cost: 0.419007\n",
      "Epoch  620 / 1000 Cost: 0.409859\n",
      "Epoch  640 / 1000 Cost: 0.400721\n",
      "Epoch  660 / 1000 Cost: 0.391591\n",
      "Epoch  680 / 1000 Cost: 0.382465\n",
      "Epoch  700 / 1000 Cost: 0.373343\n",
      "Epoch  720 / 1000 Cost: 0.364224\n",
      "Epoch  740 / 1000 Cost: 0.355108\n",
      "Epoch  760 / 1000 Cost: 0.345996\n",
      "Epoch  780 / 1000 Cost: 0.336892\n",
      "Epoch  800 / 1000 Cost: 0.327802\n",
      "Epoch  820 / 1000 Cost: 0.318732\n",
      "Epoch  840 / 1000 Cost: 0.309698\n",
      "Epoch  860 / 1000 Cost: 0.300718\n",
      "Epoch  880 / 1000 Cost: 0.291824\n",
      "Epoch  900 / 1000 Cost: 0.283062\n",
      "Epoch  920 / 1000 Cost: 0.274508\n",
      "Epoch  940 / 1000 Cost: 0.266280\n",
      "Epoch  960 / 1000 Cost: 0.258562\n",
      "Epoch  980 / 1000 Cost: 0.251622\n",
      "Epoch 1000 / 1000 Cost: 0.245773\n"
     ]
    }
   ],
   "source": [
    "#optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    \n",
    "    #H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    \n",
    "    # cost 계산\n",
    "    cost = F.cross_entropy(prediction, y_train)\n",
    "    \n",
    "    #cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #100번 마다 로그 출력\n",
    "    if epoch % 20 == 0:\n",
    "        print('Epoch {:4d} / {} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
